"ä»»åŠ¡1 å¤„ç†åˆå§‹æ•°æ®ï¼Œç¡®ä¿ä¸ä½œè€…çš„ç”¨æˆ·å•†å“IDåŒ¹é…"
import gzip
import ast
import json
# 1. åŠ è½½ user_sequence.txt ä¸­çš„å•†å“ ASIN
# sequence_file = "user_sequence.txt"
# target_asins = set()
#
# with open(sequence_file, 'r', encoding='utf-8') as f:
#     for line in f:
#         parts = line.strip().split()
#         if len(parts) > 1:
#             target_asins.update(parts[1:])
#
# print(f"âœ… ç›®æ ‡å•†å“ ASIN æ•°é‡: {len(target_asins)}")
#
# # 2. ç­›é€‰å¹¶ä¿å­˜ meta_Electronics.json.gz ä¸­åŒ¹é…å•†å“
# meta_input = "meta_Electronics.json.gz"
# meta_output = "filtered_meta_Electronics.json.gz"
#
# with gzip.open(meta_input, 'rt', encoding='utf-8', errors='ignore') as fin, \
#         gzip.open(meta_output, 'wt', encoding='utf-8') as fout:
#     kept = 0
#     for line in fin:
#         try:
#             record = ast.literal_eval(line)
#             if record.get("asin") in target_asins:
#                 fout.write(json.dumps(record) + "\n")
#                 kept += 1
#         except Exception:
#             continue
#
# print(f"âœ… å·²ä¿å­˜ {kept} æ¡å•†å“è®°å½•è‡³ {meta_output}")
#
# # 1. æå– user_sequence.txt ä¸­çš„ç”¨æˆ·å’Œå•†å“
# target_users = set()
# target_asins = set()
#
# with open(sequence_file, 'r', encoding='utf-8') as f:
#     for line in f:
#         parts = line.strip().split()
#         if parts:
#             target_users.add(parts[0])
#             target_asins.update(parts[1:])
#
# print(f"âœ… ç›®æ ‡ç”¨æˆ·æ•°: {len(target_users)}ï¼Œå•†å“æ•°: {len(target_asins)}")
#
# # 2. ç­›é€‰ reviews_Beauty_5.json.gz
# review_input = "reviews_Electronics_5.json.gz"
# review_output = "filtered_reviews_Electronics_5.json.gz"
#
# with gzip.open(review_input, 'rt', encoding='utf-8', errors='ignore') as fin, \
#      gzip.open(review_output, 'wt', encoding='utf-8') as fout:
#
#     kept = 0
#     for line in fin:
#         try:
#             review = json.loads(line)
#             if review.get("reviewerID") in target_users and review.get("asin") in target_asins:
#                 fout.write(json.dumps(review) + "\n")
#                 kept += 1
#         except Exception:
#             continue
#
# print(f"âœ… å·²ä¿å­˜ {kept} æ¡è¯„è®ºè‡³ {review_output}")

"ä»»åŠ¡1.5 å‰”é™¤å™ªå£°æ•°æ®ï¼Œç¼©å°æ•°æ®é›†çš„æ•°é‡"
from collections import defaultdict
# def load_user_sequence(file_path):
#     user_item_dict = {}
#     with open(file_path, 'r') as f:
#         for line in f:
#             parts = line.strip().split()
#             if len(parts) >= 2:
#                 user_id = parts[0]
#                 item_ids = parts[1:]
#                 user_item_dict[user_id] = item_ids
#     return user_item_dict
#
# def summarize(user_item_dict, stage="å‰"):
#     all_items = set()
#     total_interactions = 0
#     for items in user_item_dict.values():
#         all_items.update(items)
#         total_interactions += len(items)
#     print(f"ğŸ“Š è¿‡æ»¤{stage}ç»Ÿè®¡ï¼š")
#     print(f"ç”¨æˆ·æ•°: {len(user_item_dict)}")
#     print(f"é¡¹ç›®æ•°: {len(all_items)}")
#     print(f"æ€»äº¤äº’æ•°: {total_interactions}\n")
# def k_core_filter(user_item_dict, k_user=5, k_item=5):
#     user_item = defaultdict(set)
#     item_user = defaultdict(set)
#
#     # æ„å»ºåå‘ç´¢å¼•
#     for user, items in user_item_dict.items():
#         for item in items:
#             user_item[user].add(item)
#             item_user[item].add(user)
#
#     changed = True
#     while changed:
#         changed = False
#
#         # è¿‡æ»¤ç”¨æˆ·
#         users_to_remove = [u for u, items in user_item.items() if len(items) < k_user]
#         for u in users_to_remove:
#             for i in user_item[u]:
#                 item_user[i].discard(u)
#             del user_item[u]
#             changed = True
#
#         # è¿‡æ»¤é¡¹ç›®
#         items_to_remove = [i for i, users in item_user.items() if len(users) < k_item]
#         for i in items_to_remove:
#             for u in item_user[i]:
#                 user_item[u].discard(i)
#             del item_user[i]
#             changed = True
#
#     # æ„é€ æœ€ç»ˆè¿‡æ»¤åçš„ user-item å­—å…¸
#     filtered_user_item_dict = {
#         u: list(items) for u, items in user_item.items() if len(items) > 0
#     }
#
#     return filtered_user_item_dict
#
# def save_user_sequence(user_item_dict, output_path):
#     with open(output_path, 'w') as f:
#         for user, items in user_item_dict.items():
#             line = user + ' ' + ' '.join(items) + '\n'
#             f.write(line)
#
# # ========== å‚æ•°è®¾å®š ==========
# input_file = 'user_sequence_all.txt'
# output_file = 'user_sequence.txt'
# k_user = 12
# k_item = 10
#
# # ========== æ‰§è¡Œæµç¨‹ ==========
# user_item_dict =load_user_sequence(input_file)
# summarize(user_item_dict, stage="å‰")
#
# filtered_user_item_dict = k_core_filter(user_item_dict, k_user=k_user, k_item=k_item)
# summarize(filtered_user_item_dict, stage="å")
#
# save_user_sequence(filtered_user_item_dict, output_file)
# print(f"âœ… å·²ä¿å­˜è¿‡æ»¤åçš„äº¤äº’åºåˆ—è‡³ï¼š{output_file}")

"ä»»åŠ¡1.6 è°ƒæ•´è¿‡æ»¤åçš„user_sequence.txtä¸­çš„ç”¨æˆ·é¡¹ç›®äº¤äº’é¡ºåº"
from collections import defaultdict
# USER_SEQ_FILE = "user_sequence.txt"
# USER_INDEX_FILE = "user_indexing.txt"
# ITEM_COLLAB_FILE_OLD = "item_collaborative_indexing_500_20_sequential-old.txt"
# USER_SEQ_COLLAB_FILE_OLD = "user_sequence_collaborative_indexing_500_20_sequential-old.txt"
# OUTPUT_FILE = "user_sequence_reordered.txt"
#
# def read_user_sequence(path):
#     """è¿”å›ï¼šlist of (orig_user, [asin...])"""
#     seqs = []
#     with open(path, "r", encoding="utf-8") as f:
#         for line in f:
#             parts = line.strip().split()
#             if not parts:
#                 continue
#             seqs.append((parts[0], parts[1:]))
#     return seqs
#
# def read_user_index_map(path):
#     """åŸå§‹ç”¨æˆ·ID -> æ–°ç”¨æˆ·ID"""
#     m = {}
#     with open(path, "r", encoding="utf-8") as f:
#         for line in f:
#             parts = line.strip().split()
#             if len(parts) == 2:
#                 m[parts[0]] = parts[1]
#     return m
#
# def read_item_collab_map(path):
#     """ASIN -> tokenï¼ˆä¾‹å¦‚ <CI1><CI29><CI91>ï¼‰"""
#     m = {}
#     with open(path, "r", encoding="utf-8") as f:
#         for line in f:
#             parts = line.strip().split()
#             if len(parts) >= 2:
#                 asin = parts[0]
#                 token = "".join(parts[1:])
#                 m[asin] = token
#     return m
#
# def invert_item_collab_map(asin2tok):
#     """token -> [ASIN...]ï¼ˆè€ƒè™‘åˆ°ç†è®ºä¸Šå¯èƒ½ä¸€å¯¹å¤šï¼Œè¿™é‡Œå­˜listï¼‰"""
#     tok2asins = defaultdict(list)
#     for a, t in asin2tok.items():
#         tok2asins[t].append(a)
#     return tok2asins
#
# def read_user_seq_collab_old(path):
#     """æ–°ç”¨æˆ·ID + tokenâ€¦  -> dict: new_user -> [tokenâ€¦]"""
#     d = {}
#     with open(path, "r", encoding="utf-8") as f:
#         for line in f:
#             parts = line.strip().split()
#             if not parts:
#                 continue
#             new_uid = parts[0]
#             tokens = parts[1:]
#             d[new_uid] = tokens
#     return d
#
# from collections import Counter
#
# def reorder_user_items_by_tokens(user_items, token_order, tok2asins):
#     """
#     ä¿ç•™é‡å¤ï¼šåŒä¸€ ASIN å‡ºç° n æ¬¡ï¼Œå°±åœ¨ç»“æœä¸­å‡ºç° n æ¬¡ã€‚
#     ç­–ç•¥ï¼š
#       1) ç¬¬ä¸€é˜¶æ®µï¼šæŒ‰ token_order é€ä¸ª tokenï¼Œåœ¨å€™é€‰ ASIN ä¸­æŒ‘â€œè¿˜å‰©ä½™æ¬¡æ•°>0â€çš„é‚£ä¸ªï¼Œæ”¾å…¥ç»“æœå¹¶å°†å‰©ä½™æ¬¡æ•°-1ï¼›
#          è‹¥ä¸€ä¸ª token å¯¹åº”å¤šä¸ª ASINï¼Œä¼˜å…ˆé€‰åœ¨è¯¥ç”¨æˆ·åŸåºåˆ—é‡Œâ€œæ›´é å‰å‡ºç°çš„é‚£ä¸ªâ€ï¼ˆå¯é€‰ä¼˜åŒ–è§æ³¨é‡Šï¼‰ã€‚
#       2) ç¬¬äºŒé˜¶æ®µï¼šæŠŠå‰©ä½™æœªåŒ¹é…çš„ ASINï¼ŒæŒ‰åŸåºåˆ—ç›¸å¯¹é¡ºåºã€é€ä¸ªæ¶ˆè€—å‰©ä½™æ¬¡æ•°è¡¥é½ã€‚
#     """
#     # ç»Ÿè®¡æ¯ä¸ª ASIN çš„å‰©ä½™æ¬¡æ•°
#     remaining = Counter(user_items)
#     result = []
#
#     # å¯é€‰ä¼˜åŒ–ï¼šé¢„å…ˆè®°å½•æ¯ä¸ª ASIN åœ¨ç”¨æˆ·åºåˆ—ä¸­çš„â€œé¦–æ¬¡å‡ºç°ä½ç½®â€ï¼Œç”¨äº tie-break
#     first_pos = {}
#     for idx, a in enumerate(user_items):
#         if a not in first_pos:
#             first_pos[a] = idx
#
#     # é˜¶æ®µ1ï¼šæŒ‰ token é¡ºåºæ”¾ç½®
#     for tok in token_order:
#         cands = tok2asins.get(tok, [])
#         # åœ¨å€™é€‰ä¸­ï¼Œé€‰â€œè¿˜å‰©ä½™æ¬¡æ•°>0â€çš„ï¼Œè‹¥æœ‰å¤šä¸ªï¼Œé€‰ first_pos æœ€å°çš„é‚£ä¸ª
#         best = None
#         best_pos = 10**18
#         for a in cands:
#             if remaining[a] > 0:
#                 pos = first_pos.get(a, 10**18)
#                 if pos < best_pos:
#                     best = a
#                     best_pos = pos
#         if best is not None:
#             result.append(best)
#             remaining[best] -= 1
#
#     # é˜¶æ®µ2ï¼šæŠŠå‰©ä½™çš„æŒ‰åŸç›¸å¯¹é¡ºåºè¡¥é½ï¼ˆä¿ç•™é‡å¤ï¼‰
#     for a in user_items:
#         while remaining[a] > 0:
#             result.append(a)
#             remaining[a] -= 1
#
#     # å¼ºæ ¡éªŒï¼ˆé•¿åº¦åº”ä¸è¾“å…¥ä¸€è‡´ï¼‰
#     assert len(result) == len(user_items), "é‡æ’åé•¿åº¦ä¸åŸåºåˆ—ä¸ä¸€è‡´ï¼Œæ£€æŸ¥é€»è¾‘ï¼"
#     return result
#     "å»ºç«‹è¯¥ç”¨æˆ· ASIN -> å‡ºç°æ¬¡æ•°ï¼ˆä»¥ä¾¿å¤„ç†é‡å¤ï¼‰,è¿™é‡Œå‡è®¾ user_items é‡Œä¸å¸¸æœ‰é‡å¤ï¼›è‹¥æœ‰éœ€è¦ï¼Œå¯æ”¹æˆ multiset é€»è¾‘,æš‚ç”¨ set + used æ§åˆ¶ä¸€æ¬¡æ€§åŒ¹é…"
#     for tok in token_order:
#         candidates = tok2asins.get(tok, [])
#         picked = None
#         for asin in candidates:
#             if asin in remaining and asin not in used:
#                 picked = asin
#                 break
#         if picked is not None:
#             result.append(picked)
#             used.add(picked)
#             # ä¸ä» remaining ç§»é™¤æ˜¯ä¸ºäº†åç»­ä¿æŒåŸç›¸å¯¹é¡ºåºæ—¶ä»èƒ½åˆ¤æ–­
#             # ä½†æˆ‘ä»¬ç”¨ used æ§åˆ¶ä¸è¦é‡å¤åŠ å…¥
#
#     # å°†æœªåŒ¹é…åˆ° token çš„ASINï¼ŒæŒ‰åŸç›¸å¯¹é¡ºåºè¿½åŠ ï¼ˆä¸ä¸¢å¤±ï¼‰
#     for asin in user_items:
#         if asin not in used:
#             result.append(asin)
#             used.add(asin)
#
#     # ä¿é™©ï¼šé•¿åº¦åº”ä¸åŸå§‹ç›¸åŒ
#     # å¦‚æœä½ å¸Œæœ›å¼ºæ ¡éªŒï¼Œå¯åŠ  assert len(result) == len(user_items)
#     return result
#
# def main():
#     # è¯»å–æ•°æ®
#     raw_user_seqs = read_user_sequence(USER_SEQ_FILE)                 # [(orig_user, [asin...])]
#     user_id_map = read_user_index_map(USER_INDEX_FILE)                # orig_user -> new_user
#     asin2tok_old = read_item_collab_map(ITEM_COLLAB_FILE_OLD)         # asin -> token
#     tok2asins_old = invert_item_collab_map(asin2tok_old)              # token -> [asin...]
#     old_user_tok_seq = read_user_seq_collab_old(USER_SEQ_COLLAB_FILE_OLD)  # new_user -> [token...]
#
#     not_found_user = 0
#     reordered_lines = 0
#
#     with open(OUTPUT_FILE, "w", encoding="utf-8") as fout:
#         for orig_user, asins in raw_user_seqs:
#             new_user = user_id_map.get(orig_user)
#             if new_user is None:
#                 # æ‰¾ä¸åˆ°ç”¨æˆ·æ˜ å°„ï¼šä¿ç•™åŸé¡ºåºå†™å‡ºï¼Œä¹Ÿå¯é€‰æ‹©è·³è¿‡
#                 fout.write(orig_user + " " + " ".join(asins) + "\n")
#                 not_found_user += 1
#                 continue
#
#             token_order = old_user_tok_seq.get(new_user)
#             if token_order is None:
#                 # old æ–‡ä»¶é‡Œæ²¡æœ‰è¿™ä¸ªç”¨æˆ·ï¼šåŒæ ·ä¿ç•™åŸé¡ºåº
#                 fout.write(orig_user + " " + " ".join(asins) + "\n")
#                 continue
#
#             reordered = reorder_user_items_by_tokens(asins, token_order, tok2asins_old)
#             fout.write(orig_user + " " + " ".join(reordered) + "\n")
#             reordered_lines += 1
#
#     print(f"âœ… å®Œæˆé‡æ’ï¼š{reordered_lines} è¡Œå·²æŒ‰ old token é¡ºåºé‡æ’ã€‚")
#     if not_found_user:
#         print(f"âš ï¸ {not_found_user} ä¸ªç”¨æˆ·åœ¨ user_indexing.txt ä¸­æœªæ‰¾åˆ°æ˜ å°„ï¼Œå·²æŒ‰åŸé¡ºåºä¿ç•™ã€‚")
#     print(f"å·²è¾“å‡ºåˆ°ï¼š{OUTPUT_FILE}")
#
# if __name__ == "__main__":
#     main()


"ä»»åŠ¡2.é¢„å¤„ç†æ•°æ®ï¼Œæ–¹ä¾¿åç»­çš„GCNå¤„ç†åŠèšç±»"
import json
import numpy as np
import torch
from itertools import combinations
from collections import defaultdict
import gzip

# æ–‡ä»¶è·¯å¾„
# user_sequence_path = 'user_sequence.txt'
# meta_file_path = 'filtered_meta_Electronics.json.gz'
#
# # ----------------------- Step 1: è¯»å–ç”¨æˆ·åºåˆ— -----------------------
# user_sequences = {}
# all_asins_in_seq = set()
#
# with open(user_sequence_path, 'r') as f:
#     for line in f:
#         tokens = line.strip().split()
#         if len(tokens) < 2:
#             continue
#         user_id = tokens[0]
#         asins = tokens[1:]
#         user_sequences[user_id] = asins
#         all_asins_in_seq.update(asins)
#
# # ----------------------- Step 2: è¯»å–å•†å“å…ƒæ•°æ® -----------------------
# asin2meta = {}
# brands = set()
# categories = set()
#
# with gzip.open(meta_file_path, 'rt', encoding='utf-8') as f:
#     for line in f:
#         meta = json.loads(line)
#         asin = meta.get('asin')
#         if asin not in all_asins_in_seq:
#             continue
#         asin2meta[asin] = meta
#         brand = meta.get('brand')
#         if brand:
#             brands.add(brand.strip())
#         cat_list = meta.get('categories')
#         if isinstance(cat_list, list) and len(cat_list) > 0:
#             for cat in cat_list[0]:  # åªå–ç¬¬ä¸€çº§åˆ—è¡¨
#                 categories.add(cat.strip())
#
# valid_asins = sorted(asin2meta.keys())
#
# print(f"âœ… æœ‰æ•ˆå•†å“æ€»æ•°: {len(valid_asins)}")
#
# # ----------------------- Step 3: æ„å»ºåµŒå…¥ç´¢å¼•å­—å…¸ -----------------------
# brand_to_idx = {b: i for i, b in enumerate(sorted(brands))}
# category_to_idx = {c: i for i, c in enumerate(sorted(categories))}
#
# # åµŒå…¥å±‚è®¾ç½®
# embedding_dim = 64  # å¯è‡ªå®šä¹‰,è‹¥æ¨¡å‹æ¬ æ‹Ÿåˆï¼ˆèšç±»ä¸æ¸…æ™°ï¼‰ï¼Œå¯æå‡ç»´åº¦,è‹¥è¿‡æ‹Ÿåˆæˆ–æ•ˆç‡ä½ï¼Œå¯é™ä¸º 32
# brand_emb = torch.nn.Embedding(len(brand_to_idx), embedding_dim)
# cat_emb = torch.nn.Embedding(len(category_to_idx), embedding_dim)
#
# # ----------------------- Step 4: æ„å»ºå•†å“ç‰¹å¾çŸ©é˜µ -----------------------
# asin_to_index = {asin: idx for idx, asin in enumerate(valid_asins)}
# index_to_asin = {idx: asin for asin, idx in asin_to_index.items()}
#
# item_features = []
#
# for asin in valid_asins:
#     meta = asin2meta[asin]
#
#     # å“ç‰Œå‘é‡
#     brand_vec = torch.zeros(embedding_dim)
#     brand = meta.get('brand')
#     if brand and brand.strip() in brand_to_idx:
#         brand_vec = brand_emb(torch.tensor(brand_to_idx[brand.strip()]))
#
#     # ç±»åˆ«å¹³å‡å‘é‡
#     cat_vecs = []
#     cats = meta.get('categories')
#     if isinstance(cats, list) and len(cats) > 0:
#         for c in cats[0]:
#             c = c.strip()
#             if c in category_to_idx:
#                 cat_vecs.append(cat_emb(torch.tensor(category_to_idx[c])))
#     if cat_vecs:
#         cat_vec = torch.stack(cat_vecs).mean(dim=0)
#     else:
#         cat_vec = torch.zeros(embedding_dim)
#
#     # åˆå¹¶å‘é‡
#     final_vec = torch.cat([brand_vec, cat_vec])
#     item_features.append(final_vec.detach().numpy())
#
# item_features = np.array(item_features, dtype=np.float32)
# np.save('item_features.npy', item_features)
# print(f"ğŸ“¦ item_features.npy ä¿å­˜å®Œæˆï¼Œå½¢çŠ¶: {item_features.shape}")
#
# # ----------------------- Step 5: æ„å»ºå•†å“å…±ç°å›¾ -----------------------
# co_occur_dict = defaultdict(int)
# for asins in user_sequences.values():
#     filtered = [a for a in asins if a in asin_to_index]
#     for i, j in combinations(filtered, 2):
#         if i != j:
#             pair = tuple(sorted([i, j]))
#             co_occur_dict[pair] += 1
#
# edge_index = []
# edge_weight = []
#
# for (a1, a2), w in co_occur_dict.items():
#     edge_index.append([asin_to_index[a1], asin_to_index[a2]])
#     edge_weight.append(w)
#
# edge_index = torch.tensor(edge_index, dtype=torch.long).T
# edge_weight = torch.tensor(edge_weight, dtype=torch.float)
#
# np.savez('movie_graph_edges.npz', edge_index=edge_index, edge_weight=edge_weight)
# print(f"ğŸ§© å…±ç°å›¾ä¿å­˜å®Œæˆï¼Œå…±æœ‰è¾¹æ•°: {edge_index.shape[1]}")
#
# # ----------------------- Step 6: ä¿å­˜ç´¢å¼•æ˜ å°„å­—å…¸ -----------------------
# with open('asin_to_index.json', 'w', encoding='utf-8') as f:
#     json.dump(asin_to_index, f, ensure_ascii=False, indent=2)
# with open('index_to_asin.json', 'w', encoding='utf-8') as f:
#     json.dump({str(k): v for k, v in index_to_asin.items()}, f, ensure_ascii=False, indent=2)
#
# print("âœ… æ‰€æœ‰å¤„ç†å®Œæˆï¼å¯è¿›è¡ŒGCNèšç±»åˆ†æã€‚")


"ä»»åŠ¡3.é€šè¿‡ä½œè€…æä¾›çš„æ˜ å°„IDè¿˜æœ‰æˆ‘ä»¬æ–°çš„ç´¢å¼•IDï¼Œæ˜ å°„å‡ºæ–°çš„user_sequence_collaborative_indexing_500_20_sequential.txtï¼Œå³åŒ…æ‹¬æ˜ å°„ç”¨æˆ·IDå’Œç‰©å“ç´¢å¼•ID"
# å®šä¹‰å‡½æ•°è¯»å–ç”¨æˆ·æ˜ å°„æ–‡ä»¶ï¼Œå¹¶è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œé”®ä¸ºåŸå§‹ç”¨æˆ·IDï¼Œå€¼ä¸ºæ–°çš„ç”¨æˆ·ID
def read_user_mapping(file_path):
    user_mapping = {}
    with open(file_path, 'r') as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) == 2:
                original_user_id = parts[0]
                new_user_id = parts[1]
                user_mapping[original_user_id] = new_user_id
    return user_mapping

# --------------------------- æ›¿æ¢ç”¨æˆ·ID --------------------------- #
def remap_user_ids(user_sequence_file, user_mapping, output_file):
    with open(user_sequence_file, 'r') as f_in, open(output_file, 'w') as f_out:
        for line in f_in:
            parts = line.strip().split()
            if parts:
                original_user_id = parts[0]
                if original_user_id in user_mapping:
                    new_user_id = user_mapping[original_user_id]
                    new_line = f"{new_user_id} " + " ".join(parts[1:]) + "\n"
                    f_out.write(new_line)
                else:
                    print(f"âš ï¸ è·³è¿‡ä¸€è¡Œï¼šç”¨æˆ· {original_user_id} æœªæ‰¾åˆ°æ˜ å°„")

# --------------------------- è¯»å–ç‰©å“æ˜ å°„ --------------------------- #
def read_item_mapping(file_path):
    item_mapping = {}
    with open(file_path, 'r') as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 2:
                original_item_id = parts[0]
                new_item_id = "".join(parts[1:])  # æ‹¼æ¥å®Œæ•´è·¯å¾„ <CIx><CIy>
                item_mapping[original_item_id] = new_item_id
    return item_mapping

# --------------------------- æ›¿æ¢ç‰©å“ID --------------------------- #
def remap_item_ids(user_sequence_file, item_mapping, output_file):
    with open(user_sequence_file, 'r') as f_in, open(output_file, 'w') as f_out:
        for line in f_in:
            parts = line.strip().split()
            if parts:
                user_id = parts[0]
                try:
                    new_item_ids = [item_mapping[item_id] for item_id in parts[1:]]
                    new_line = f"{user_id} " + " ".join(new_item_ids) + "\n"
                    f_out.write(new_line)
                except KeyError as e:
                    print(f"âš ï¸ è·³è¿‡ä¸€è¡Œï¼šç‰©å“IDæœªæ˜ å°„ {e}")

# --------------------------- æŒ‰ç”¨æˆ·IDæ’åº --------------------------- #
def sort_user_sequence(input_file, output_file):
    with open(input_file, 'r') as f_in:
        lines = f_in.readlines()

    # è½¬æ¢ä¸ºæ•´æ•°IDæ’åº
    sorted_lines = sorted(lines, key=lambda line: int(line.split()[0]))

    with open(output_file, 'w') as f_out:
        for line in sorted_lines:
            f_out.write(line)

# --------------------------- ä¸»æ‰§è¡Œæµç¨‹ --------------------------- #
k = 2
n = 500

# æ–‡ä»¶è·¯å¾„é…ç½®
user_sequence_file = 'user_sequence.txt'                         # åŸå§‹å­—ç¬¦ä¸²ç”¨æˆ·åºåˆ—
user_indexing_file = 'user_indexing.txt'                         # ç”¨æˆ·æ˜ å°„æ–‡ä»¶ï¼šå­—ç¬¦ä¸²ID â æ•´æ•°ID
user_sequence_mapped = 'user_sequence_mapped.txt'                # æ›¿æ¢ç”¨æˆ·IDåçš„ä¸­é—´æ–‡ä»¶

item_collaborative_indexing_file = f'item_collaborative_indexing_{n}_{k}_sequential.txt'  # ç‰©å“æ˜ å°„è·¯å¾„
user_sequence_final = f'user_sequence_final_{n}_{k}.txt'         # æ›¿æ¢ç‰©å“IDåçš„æ–‡ä»¶
user_sequence_sorted = f'user_sequence_collaborative_indexing_{n}_{k}_sequential.txt'  # æœ€ç»ˆè¾“å‡ºæ–‡ä»¶

# Step 1ï¼šæ›¿æ¢ç”¨æˆ·ID
user_mapping = read_user_mapping(user_indexing_file)
remap_user_ids(user_sequence_file, user_mapping, user_sequence_mapped)
print(f"âœ… ç”¨æˆ·IDå·²æ˜ å°„å¹¶ä¿å­˜è‡³: {user_sequence_mapped}")

# Step 2ï¼šæ›¿æ¢ç‰©å“ID
item_mapping = read_item_mapping(item_collaborative_indexing_file)
remap_item_ids(user_sequence_mapped, item_mapping, user_sequence_final)
print(f"âœ… ç‰©å“IDå·²æ˜ å°„å¹¶ä¿å­˜è‡³: {user_sequence_final}")

# Step 3ï¼šæ’åºè¾“å‡º
sort_user_sequence(user_sequence_final, user_sequence_sorted)
print(f"âœ… æœ€ç»ˆæ–‡ä»¶å·²æ’åºå¹¶ä¿å­˜è‡³: {user_sequence_sorted}")


"ä»»åŠ¡4. æ ¹æ®è¿‡æ»¤åçš„æ•°æ®é›†æ¸…é™¤æ‰ä½œè€…çš„ååŒç´¢å¼•æ•°æ®ä¸­ä¸å­˜åœ¨çš„uç”¨æˆ·å’Œé¡¹ç›®"
import re

# æ–‡ä»¶è·¯å¾„å®šä¹‰
# user_sequence_txt = 'user_sequence.txt'
# user_indexing_txt = 'user_indexing.txt'
# user_seq_old_txt = 'user_sequence_collaborative_indexing_500_20_sequential-old-all.txt'
# item_index_old_txt = 'item_collaborative_indexing_500_20_sequential-old-all.txt'
#
# user_seq_clean_txt = 'user_sequence_collaborative_indexing_500_20_sequential-old.txt'
# item_index_clean_txt = 'item_collaborative_indexing_500_20_sequential-old.txt'
#
# # Step 1: åŠ è½½åŸå§‹ç”¨æˆ·å’Œé¡¹ç›®é›†åˆ
# def load_user_item_set(user_sequence_file):
#     user_set = set()
#     item_set = set()
#     with open(user_sequence_file, 'r') as f:
#         for line in f:
#             parts = line.strip().split()
#             if parts:
#                 user_set.add(parts[0])
#                 item_set.update(parts[1:])
#     return user_set, item_set
#
# # Step 2: è¯»å–ç”¨æˆ·IDæ˜ å°„
# def load_user_indexing(file_path):
#     new_to_raw = {}
#     with open(file_path, 'r') as f:
#         for line in f:
#             raw_id, new_id = line.strip().split()
#             new_to_raw[new_id] = raw_id
#     return new_to_raw
#
# # Step 3: è¯»å– item token â†’ åŸå§‹ item çš„æ˜ å°„ï¼ˆåå‘ï¼‰
# def load_token_to_item(file_path):
#     token_to_item = {}
#     with open(file_path, 'r') as f:
#         for line in f:
#             parts = line.strip().split()
#             if len(parts) == 2:
#                 raw_item, token = parts
#                 token_to_item[token] = raw_item
#     return token_to_item
#
# # Step 4: è¿‡æ»¤ç”¨æˆ·äº¤äº’æ–‡ä»¶
# def filter_user_sequence(user_seq_file, new_to_raw_user, valid_users, valid_items, token_to_item, output_file):
#     retained_lines = 0
#     with open(user_seq_file, 'r') as f_in, open(output_file, 'w') as f_out:
#         for line in f_in:
#             parts = line.strip().split()
#             if not parts:
#                 continue
#
#             new_user_id = parts[0]
#             raw_user_id = new_to_raw_user.get(new_user_id)
#             if raw_user_id not in valid_users:
#                 continue
#
#             # æå–åŸå§‹é¡¹ç›®IDï¼Œè¿‡æ»¤ä¸åœ¨ valid_items ä¸­çš„é¡¹ç›®
#             filtered_tokens = []
#             for token in parts[1:]:
#                 if token in token_to_item and token_to_item[token] in valid_items:
#                     filtered_tokens.append(token)
#
#             if filtered_tokens:
#                 f_out.write(f"{new_user_id} {' '.join(filtered_tokens)}\n")
#                 retained_lines += 1
#     print(f"âœ… è¿‡æ»¤åç”¨æˆ·äº¤äº’åºåˆ—ä¿ç•™ {retained_lines} æ¡è®°å½•")
#
# # Step 5: è¿‡æ»¤é¡¹ç›®æ˜ å°„æ–‡ä»¶
# def filter_item_indexing(item_index_file, valid_items, output_file):
#     retained = 0
#     with open(item_index_file, 'r') as f_in, open(output_file, 'w') as f_out:
#         for line in f_in:
#             parts = line.strip().split()
#             if parts and parts[0] in valid_items:
#                 f_out.write(line)
#                 retained += 1
#     print(f"âœ… è¿‡æ»¤åä¿ç•™é¡¹ç›® {retained} ä¸ª")
#
# # æ‰§è¡Œæµç¨‹
# valid_users, valid_items = load_user_item_set(user_sequence_txt)
# new_to_raw_user = load_user_indexing(user_indexing_txt)
# token_to_item = load_token_to_item(item_index_old_txt)
#
# filter_user_sequence(user_seq_old_txt, new_to_raw_user, valid_users, valid_items, token_to_item, user_seq_clean_txt)
# filter_item_indexing(item_index_old_txt, valid_items, item_index_clean_txt)



"ä»»åŠ¡5. æ ¹æ®è¿‡æ»¤åçš„æ•°æ®é›†æ¸…é™¤æ‰ä½œè€…çš„æ•°æ®ä¸­éšæœºç´¢å¼•å’Œé¡ºåºç´¢å¼•ä¸å­˜åœ¨çš„uç”¨æˆ·å’Œé¡¹ç›®"
# æ–‡ä»¶è·¯å¾„
# user_sequence_txt = 'user_sequence.txt'
# user_indexing_txt = 'user_indexing.txt'
# # éšæœºç´¢å¼•
# # user_seq_old_txt = 'user_sequence_random_indexing_old.txt'
# # item_index_old_txt = 'item_random_indexing_old.txt'
# # user_seq_clean_txt = 'user_sequence_random_indexing.txt'
# # item_index_clean_txt = 'item_random_indexing.txt'
#
# # é¡ºåºç´¢å¼•
# user_seq_old_txt = 'user_sequence_sequential_indexing_original_old.txt'
# item_index_old_txt = 'item_sequential_indexing_original_old.txt'
# user_seq_clean_txt = 'user_sequence_sequential_indexing_original.txt'
# item_index_clean_txt = 'item_sequential_indexing_original.txt'
#
#
# # Step 1: è¯»å–åŸå§‹ç”¨æˆ·åºåˆ—ï¼Œè·å–å­˜åœ¨çš„ç”¨æˆ·å’Œé¡¹ç›®é›†åˆ
# valid_users = set()
# valid_items = set()
# with open(user_sequence_txt, 'r') as f:
#     for line in f:
#         parts = line.strip().split()
#         if not parts:
#             continue
#         user_id = parts[0]
#         item_ids = parts[1:]
#         valid_users.add(user_id)
#         valid_items.update(item_ids)
#
# # Step 2: è¯»å–ç”¨æˆ·æ˜ å°„è¡¨ï¼šåŸå§‹ç”¨æˆ· â†’ æ–°ç”¨æˆ·ID
# user_id_map = {}
# with open(user_indexing_txt, 'r') as f:
#     for line in f:
#         orig, new = line.strip().split()
#         user_id_map[orig] = new
#
# # Step 3: æ„å»ºåå‘æ˜ å°„ï¼šæ–°ç”¨æˆ·ID â†’ åŸå§‹ç”¨æˆ·ID
# new_to_orig_user = {v: k for k, v in user_id_map.items()}
#
# # Step 4: è¿‡æ»¤ user_sequence_random_indexing_old.txt
# with open(user_seq_old_txt, 'r') as f_in, open(user_seq_clean_txt, 'w') as f_out:
#     for line in f_in:
#         parts = line.strip().split()
#         if not parts:
#             continue
#         new_user_id = parts[0]
#         orig_user_id = new_to_orig_user.get(new_user_id)
#         if orig_user_id in valid_users:
#             f_out.write(line)
#
# # Step 5: è¿‡æ»¤ item_random_indexing_old.txt
# with open(item_index_old_txt, 'r') as f_in, open(item_index_clean_txt, 'w') as f_out:
#     for line in f_in:
#         parts = line.strip().split(maxsplit=1)
#         if not parts:
#             continue
#         item_id = parts[0]
#         if item_id in valid_items:
#             f_out.write(line)


"ä»»åŠ¡5.æ–‡ä»¶è·¯å¾„ è¿‡æ»¤æ‰indexä¸­ä¸éœ€è¦çš„ç”¨æˆ·é¡¹ç›®"
# user_sequence_txt = 'user_sequence.txt'
# user_indexing_txt = 'user_indexing.txt'
# user_seq_old_txt = 'user_sequence_random_indexing_old.txt'
# item_index_old_txt = 'item_random_indexing_old.txt'
#
# user_seq_clean_txt = 'user_sequence_random_indexing.txt'
# item_index_clean_txt = 'item_random_indexing.txt'
# user_indexing_new_txt = 'user_indexing_new.txt'
#
# # Step 1: è¯»å–åŸå§‹ç”¨æˆ·åºåˆ—ï¼Œè·å–å­˜åœ¨çš„ç”¨æˆ·å’Œé¡¹ç›®é›†åˆ
# valid_users = set()
# valid_items = set()
# with open(user_sequence_txt, 'r') as f:
#     for line in f:
#         parts = line.strip().split()
#         if not parts:
#             continue
#         user_id = parts[0]
#         item_ids = parts[1:]
#         valid_users.add(user_id)
#         valid_items.update(item_ids)
#
# # Step 2: è¯»å–ç”¨æˆ·æ˜ å°„è¡¨ï¼šåŸå§‹ç”¨æˆ· â†’ æ–°ç”¨æˆ·ID
# user_id_map = {}
# with open(user_indexing_txt, 'r') as f:
#     for line in f:
#         orig, new = line.strip().split()
#         user_id_map[orig] = new
#
# # Step 3: æ„å»ºåå‘æ˜ å°„ï¼šæ–°ç”¨æˆ·ID â†’ åŸå§‹ç”¨æˆ·ID
# new_to_orig_user = {v: k for k, v in user_id_map.items()}
#
# # Step 4: è¿‡æ»¤ user_sequence_random_indexing_old.txt
# with open(user_seq_old_txt, 'r') as f_in, open(user_seq_clean_txt, 'w') as f_out:
#     for line in f_in:
#         parts = line.strip().split()
#         if not parts:
#             continue
#         new_user_id = parts[0]
#         orig_user_id = new_to_orig_user.get(new_user_id)
#         if orig_user_id in valid_users:
#             f_out.write(line)
#
# # Step 5: è¿‡æ»¤ item_random_indexing_old.txt
# with open(item_index_old_txt, 'r') as f_in, open(item_index_clean_txt, 'w') as f_out:
#     for line in f_in:
#         parts = line.strip().split(maxsplit=1)
#         if not parts:
#             continue
#         item_id = parts[0]
#         if item_id in valid_items:
#             f_out.write(line)
#
# # Step 6: åˆ é™¤ user_indexing.txt ä¸­ä¸å­˜åœ¨äº user_sequence.txt çš„ç”¨æˆ·
# with open(user_indexing_txt, 'r') as f_in, open(user_indexing_new_txt, 'w') as f_out:
#     for line in f_in:
#         orig, new = line.strip().split()
#         if orig in valid_users:
#             f_out.write(line)




